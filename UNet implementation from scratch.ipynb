{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"},{"sourceId":10810821,"sourceType":"datasetVersion","datasetId":6710999}],"dockerImageVersionId":30066,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset preparation","metadata":{}},{"cell_type":"code","source":"# importing libraries and modules\n\nimport pickle\nimport os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport torch.nn.functional as F","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGE_SIZE = (320, 320)\nBATCH_SIZE = 32\n# EPOCHS = 10\nLEARNING_RATE = 1e-4\nIMAGE_DIR = \"/kaggle/input/m-rose-data/images/images\"  # Path to the folder containing images\nMASK_DIR = \"/kaggle/input/m-rose-data/masked images/masked images\"    # Path to the folder containing masks\n\nlen(os.listdir(IMAGE_DIR)), len(os.listdir(MASK_DIR))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_and_preprocess_data(image_dir, mask_dir, image_size):\n    image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir)])\n    mask_paths = sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir)])\n\n    images = []\n    masks = []\n\n    for img_path, mask_path in zip(image_paths, mask_paths):\n\n        img = cv2.imread(img_path)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n\n        img = cv2.resize(img, image_size)\n        mask = cv2.resize(mask, image_size)\n\n        img = img.astype('float32') / 255.0\n        mask = mask.astype('float32') / 255.0\n\n        images.append(img)\n        masks.append(mask)\n        \n    return np.array(images), np.array(masks)\n\nimages, masks = load_and_preprocess_data(IMAGE_DIR, MASK_DIR, IMAGE_SIZE)\nX_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pickle\n\n# # pickle.dump(X_train, open(\"X_train.p\", \"wb\"))\n# # pickle.dump(X_test, open(\"X_test.p\", \"wb\"))\n# pickle.dump(y_train, open(\"y_train.p\", \"wb\"))\n# pickle.dump(y_test, open(\"y_test.p\", \"wb\"))\n\n\n# # favorite_color = pickle.load(open(\"save.p\", \"rb\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_train_loader = DataLoader(X_train, batch_size=BATCH_SIZE, shuffle=True)\nmasks_train_loader = DataLoader(y_train, batch_size=BATCH_SIZE, shuffle=True)\nimages_test_loader = DataLoader(X_test, batch_size=BATCH_SIZE, shuffle=True)\nmasks_test_loader = DataLoader(y_test, batch_size=BATCH_SIZE, shuffle=True)\n\n# # checking the dimensions\n# for images, masks in zip(images_test_loader, masks_test_loader):\n#     images = images.permute(0,3,1,2)\n#     print(images.shape, masks.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# U-Net implementation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nclass UNetModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # encoder block (downsampling)\n        self.enc_c1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.enc_c2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n\n        self.enc_c3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.enc_c4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n\n        self.enc_c5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.enc_c6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n\n        self.enc_c7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.enc_c8 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # concatenate\n\n        #bottleneck block\n        self.b1 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n        self.b2 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n\n        # decoder block\n        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.dec_c1 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n        self.dec_c2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n\n        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec_c3 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n        self.dec_c4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n\n        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec_c5 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n        self.dec_c6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n\n        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec_c7 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.dec_c8 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n\n        # output layer\n        self.output_layer = nn.Conv2d(64, 2, kernel_size=1)\n\n    def forward(self, x):\n\n        # encoder block\n        encoder1 = F.relu(self.enc_c1(x))\n        encoder1 = F.relu(self.enc_c2(encoder1)) #concat layer\n        encoder12 = F.max_pool2d(encoder1, 2)\n\n        encoder12 = F.relu(self.enc_c3(encoder12))\n        encoder12 = F.relu(self.enc_c4(encoder12)) #concat layer\n        encoder22 = F.max_pool2d(encoder12, 2)\n\n        encoder22 = F.relu(self.enc_c5(encoder22))\n        encoder22 = F.relu(self.enc_c6(encoder22)) #concat layer\n        encoder32 = F.max_pool2d(encoder22, 2)\n\n        encoder32 = F.relu(self.enc_c7(encoder32))\n        encoder32 = F.relu(self.enc_c8(encoder32)) #concat layre\n        bottleneck = F.max_pool2d(encoder32, 2)\n\n        #bottleneck block\n        bottleneck = F.relu(self.b1(bottleneck))\n        bottleneck = F.relu(self.b2(bottleneck))\n\n        # decoder block (upsampling)\n        upsampling1 = self.up1(bottleneck)\n        concat1 = torch.cat([encoder32, upsampling1], dim=1)\n        decoder1 = F.relu(self.dec_c1(concat1))\n        decoder1 = F.relu(self.dec_c2(decoder1))\n\n        upsampling2 = self.up2(decoder1)\n        concat2 = torch.cat([encoder22, upsampling2], dim=1)\n        decoder2 = F.relu(self.dec_c3(concat2))\n        decoder2 = F.relu(self.dec_c4(decoder2))\n\n        upsampling3 = self.up3(decoder2)\n        concat3 = torch.cat([encoder12, upsampling3], dim=1)\n        decoder3 = F.relu(self.dec_c5(concat3))\n        decoder3 = F.relu(self.dec_c6(decoder3))\n\n        upsampling4 = self.up4(decoder3)  # Changed from self.up1 to self.up4\n        concat4 = torch.cat([encoder1, upsampling4], dim=1)\n        decoder4 = F.relu(self.dec_c7(concat4))\n        decoder4 = F.relu(self.dec_c8(decoder4))\n\n        return self.output_layer(decoder4)  # Removed F.relu here\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(\"cpu\")\nprint(f\"using device : {device}\")\n\nmodel = UNetModel().to(device)\ncriterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class masks\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training Loop\n\nepochs = 30\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n\n    i=1\n    for images, masks in zip(images_train_loader, masks_train_loader):\n        images, masks = images.to(device), masks.to(device)\n\n        images = images.permute(0, 3, 1, 2)  # Adjust this based on your data\n\n        masks = masks.long()  # Convert to long tensor\n\n        optimizer.zero_grad()\n        outputs = model(images)  # Forward pass\n\n        loss = criterion(outputs, masks)  # Compute loss\n        loss.backward()  # Backward pass\n        optimizer.step()  # Update weights\n\n        running_loss += loss.item()\n        print(f\"end of {i}th batch of {epoch+1}th epoch\")\n        i+=1\n\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(images_train_loader):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# evaluation UNet model\n\nimport torch\n\ndef calculate_accuracy(output, target):\n\n    # Convert logits to predicted class indices\n    preds = torch.argmax(output, dim=1)  # Shape: (batch_size, height, width)\n\n    # Calculate correct predictions\n    correct = (preds == target).float()  # Shape: (batch_size, height, width)\n\n    # Calculate accuracy\n    accuracy = correct.sum() / correct.numel()\n    return accuracy.item()\n\ndef calculate_iou(output, target, num_classes):\n\n    # Convert logits to predicted class indices\n    preds = torch.argmax(output, dim=1)  # Shape: (batch_size, height, width)\n\n    iou_per_class = []\n\n    for cls in range(num_classes):\n\n        pred_mask = (preds == cls)  # Shape: (batch_size, height, width)\n        target_mask = (target == cls)  # Shape: (batch_size, height, width)\n\n        # Calculate intersection and union\n        intersection = (pred_mask & target_mask).float().sum()  # TP\n        union = (pred_mask | target_mask).float().sum()  # TP + FP + FN\n\n        # Avoid division by zero\n        if union == 0:\n            iou_per_class.append(float('nan'))  # Ignore this class if no ground truth\n        else:\n            iou_per_class.append((intersection / union).item())\n\n    # Calculate mean IoU, ignoring NaN values\n    iou_per_class = torch.tensor(iou_per_class)\n    mean_iou = torch.mean(iou_per_class).item()\n    return mean_iou","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, test_images, test_masks, num_classes, device):\n\n    model.eval()  # Set model to evaluation mode\n    total_accuracy = 0.0\n    total_iou = 0.0\n    num_batches = 0\n\n    with torch.no_grad():  # Disable gradient computation\n        for images, masks in zip(test_images, test_masks):\n            # Move data to the device\n            images = images.to(device)\n            masks = masks.to(device)\n\n            images = images.permute(0, 3, 1, 2)  # Adjust this based on your data\n\n            masks = masks.long()  # Convert to long tensor\n\n            # Forward pass\n            outputs = model(images)\n\n            # Calculate metrics\n            accuracy = calculate_accuracy(outputs, masks)\n            iou = calculate_iou(outputs, masks, num_classes)\n\n            # Accumulate metrics\n            total_accuracy += accuracy\n            total_iou += iou\n            num_batches += 1\n\n    # Calculate average metrics\n    avg_accuracy = total_accuracy / num_batches\n    avg_iou = total_iou / num_batches\n\n    return avg_accuracy, avg_iou","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define your model, dataloader, and device\nmodel = UNetModel().to(device)\nnum_classes = 2  # Adjust based on your dataset\n\n# Evaluate the model\naccuracy, mean_iou = evaluate_model(model, images_test_loader, masks_test_loader, num_classes, device)\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Mean IoU: {mean_iou:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"using pre-trained libraries","metadata":{}},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\n\n# Load a pre-trained U-Net model\nmodel = smp.Unet(\n    encoder_name='resnet34',  # You can use 'efficientnet-b0', 'vgg16', etc.\n    encoder_weights='imagenet',  # Pre-trained weights\n    classes=1,  # Number of output classes\n    activation='sigmoid'  # Activation function for the output layer\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:07:51.010475Z","iopub.execute_input":"2025-02-21T14:07:51.010801Z","iopub.status.idle":"2025-02-21T14:07:51.798774Z","shell.execute_reply.started":"2025-02-21T14:07:51.010774Z","shell.execute_reply":"2025-02-21T14:07:51.797694Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import cv2\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nimport segmentation_models_pytorch as smp\n\n# Load a pre-trained U-Net model\nmodel = smp.Unet(\n    encoder_name='resnet34',  # You can use 'efficientnet-b0', 'vgg16', etc.\n    encoder_weights='imagenet',  # Pre-trained weights\n    classes=1,  # Number of output classes\n    activation='sigmoid'  # Activation function for the output layer\n)\n\n# Load and preprocess the image\nimage_path = 'your_image.jpg'  # Replace with your image path\nimage = cv2.imread(image_path, cv2.IMREAD_COLOR)  # Read the image\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\nimage = cv2.resize(image, (256, 256))  # Resize to 256x256\n\n# Convert the image to a PyTorch tensor and normalize\npreprocess = transforms.Compose([\n    transforms.ToTensor(),  # Convert to tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n])\ninput_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n\n# Move the tensor to the appropriate device (CPU or GPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ninput_tensor = input_tensor.to(device)\nmodel = model.to(device)\n\n# Perform inference\nmodel.eval()\nwith torch.no_grad():\n    output = model(input_tensor)\n\n# Get the output mask\noutput_mask = output.squeeze().cpu().numpy()  # Remove batch dimension and move to CPU\n\n# Apply a threshold to get a binary mask\nthreshold = 0.5  # You can adjust this threshold\nbinary_mask = (output_mask > threshold).astype(np.uint8)\n\n# Plot the original image and the segmentation mask\nplt.figure(figsize=(10, 5))\n\n# Original image\nplt.subplot(1, 2, 1)\nplt.title('Original Image')\nplt.imshow(image)\nplt.axis('off')\n\n# Segmentation mask\nplt.subplot(1, 2, 2)\nplt.title('Segmentation Mask')\nplt.imshow(binary_mask, cmap='gray')\nplt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}